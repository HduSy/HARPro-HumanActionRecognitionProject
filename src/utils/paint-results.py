import matplotlib.pyplot as plt

# 未加注意力
without_attention = None
# 加注意力
with_attention = None
ten = None
twenty = None
thirty = None
forty = None
other = {'val_loss': [0.6741411634814578, 0.33933636177416016, 0.23327548223256042, 0.1865627692501681, 0.1671832562603993, 0.14662269255344879, 0.14629158633784153, 0.1405732474756571, 0.13744824413850495, 0.14586385950342887, 0.15349017336340243, 0.16567427970540283, 0.14880095558448106, 0.15848689187343898, 0.1567136939653112, 0.17496045678608177, 0.17486228815736804, 0.16645186027454803, 0.16773638383896192, 0.16272284440064502, 0.18709464219292174, 0.16697002632198693, 0.17276890302542136, 0.17954418402385178, 0.1857997763278814, 0.1867795372679854, 0.20044710971623983], 'val_accuracy': [0.7832398414611816, 0.8766083717346191, 0.9135598540306091, 0.9293962121009827, 0.9353348612785339, 0.9406136870384216, 0.9435829520225525, 0.9472121596336365, 0.9501814842224121, 0.9498515129089355, 0.949191689491272, 0.9452325701713562, 0.9518311023712158, 0.949191689491272, 0.9501814842224121, 0.9478719830513, 0.9515011310577393, 0.9511712193489075, 0.9524909257888794, 0.955460250377655, 0.9485318660736084, 0.9534807205200195, 0.9548003673553467, 0.9548003673553467, 0.953150749206543, 0.9541405439376831, 0.9518311023712158], 'loss': [1.2897920932368212, 0.5530189767109245, 0.2961652005877733, 0.2116573368353759, 0.16050466529781537, 0.11524320357583477, 0.09478617554015405, 0.07953839148727108, 0.06597150875813562, 0.05221083074253355, 0.04261416017894417, 0.0318433785947487, 0.025587622009046948, 0.020313210506491826, 0.017915119092843046, 0.016005503506720833, 0.01852458226759636, 0.011790164926521924, 0.01476750831504903, 0.01435516375774232, 0.015011107473361608, 0.012213140080439618, 0.008201280846202951, 0.006720172469504935, 0.007770858807321211, 0.0051142856081202144, 0.004943718272963249], 'accuracy': [0.54934156, 0.82588196, 0.90668184, 0.9326939, 0.9461876, 0.96439606, 0.96667206, 0.9780524, 0.98114127, 0.9865063, 0.9899203, 0.9947976, 0.9954479, 0.9967485, 0.99691105, 0.99626076, 0.99642336, 0.9982117, 0.99707365, 0.9980491, 0.9973988, 0.99772394, 0.99886197, 0.9991871, 0.99772394, 0.9991871, 0.99886197]}
best = \
    {'val_loss': [0.4579408337941041, 0.22602555306226166, 0.1750704045771207, 0.14052581189204155, 0.13953256349261592,
                  0.11983771191897186, 0.12222652921062002, 0.11969327744298991, 0.13082732337676067,
                  0.11669555555764649, 0.11934348084707447, 0.12645155034034355, 0.13451066073931958,
                  0.19411544236685072, 0.14827263081584038, 0.1426070548360545, 0.14190305246110482,
                  0.13508845533913413, 0.14282846584570913, 0.1425904692022032, 0.14255271957457544,
                  0.14666249559274458, 0.1500241839099274, 0.14892972761254514, 0.16835169552349358,
                  0.15860374622602366, 0.1568629969943692],
     'val_accuracy': [0.8514998555183411, 0.9212949275970459, 0.939708948135376, 0.9471339583396912, 0.9492129683494568,
                      0.9539649486541748, 0.9527769684791565, 0.9557469487190247, 0.9533709287643433,
                      0.9584199786186218, 0.9584199786186218, 0.9596079587936401, 0.9587169289588928,
                      0.9438669681549072, 0.9560439586639404, 0.958122968673706, 0.9602019786834717, 0.9607959389686584,
                      0.9637659788131714, 0.9631719589233398, 0.9625779390335083, 0.9637659788131714,
                      0.9628749489784241, 0.9631719589233398, 0.9607959389686584, 0.9607959389686584,
                      0.9631719589233398],
     'loss': [1.1158422616366548, 0.3854609901808102, 0.23123123076068092, 0.16163101018503997, 0.12677673771410675,
              0.10528903376941028, 0.08220652987414316, 0.06671603751952464, 0.054144526815184675, 0.04422833897814268,
              0.02755967511061411, 0.023823875709365446, 0.017037789500822548, 0.01895264227917628,
              0.025377969333281847, 0.018100696485631295, 0.011464271480255722, 0.007392995169570461,
              0.005820995866718297, 0.005127058576777765, 0.004388756040514231, 0.004311366953858163,
              0.00490966766034272, 0.0036989665428648452, 0.004903359646675155, 0.0032562580319435374,
              0.0037010703110525733],
     'accuracy': [0.6338502, 0.8700995, 0.91881216, 0.9433879, 0.9547981, 0.96518433, 0.9745465, 0.9811293, 0.986103,
                  0.9899064, 0.9951726, 0.99502635, 0.99839085, 0.995904, 0.9944412, 0.995904, 0.99765944, 0.99941486,
                  0.9992686, 0.9992686, 0.99941486, 0.9991223, 0.998976, 0.99985373, 0.99868345, 0.99941486, 0.9992686]}
# {'val_loss': [0.4395554823818071, 0.21189496566837182, 0.14649426622992812, 0.1282155886226073, 0.12233139351530337, 0.09895487354289965, 0.09882246456677407, 0.09413218935816055, 0.09034136885498853, 0.09472736054193312, 0.09948096083261943, 0.09459420016828182, 0.1021906741290957, 0.10542969689390533, 0.10415010205363533, 0.10202035863797632, 0.10827556129643583, 0.10789247267064396, 0.1071223701387179, 0.11073426782428214, 0.11492795781309174, 0.1181858220768622, 0.11984107885341692, 0.1347280419988982, 0.13527531385696143, 0.12526937235917932, 0.12984436509435382], 'val_accuracy': [0.8693198561668396, 0.9349569082260132, 0.9474309682846069, 0.9578259587287903, 0.9548559784889221, 0.9643599390983582, 0.9673299789428711, 0.9700029492378235, 0.9688149690628052, 0.9667359590530396, 0.9676269888877869, 0.9700029492378235, 0.965844988822937, 0.9670329689979553, 0.9697059988975525, 0.9702999591827393, 0.9702999591827393, 0.9708939790725708, 0.970596969127655, 0.9688149690628052, 0.970596969127655, 0.9688149690628052, 0.9702999591827393, 0.9670329689979553, 0.9694089889526367, 0.9700029492378235, 0.9688149690628052], 'loss': [1.0552818521273073, 0.36846737337782065, 0.19988443391618846, 0.1365745506217216, 0.11189412224041641, 0.10129868505536778, 0.06872142443074826, 0.05439341281888916, 0.04109033151824548, 0.029522759965231293, 0.025011868502197034, 0.021140739154953438, 0.014888009641727137, 0.019094003600952848, 0.015388267167346337, 0.00938820725414769, 0.008139894145005206, 0.005866901582840453, 0.005259727601157698, 0.004052521364197719, 0.0036720159630339666, 0.0032597468566338025, 0.002859575272308343, 0.006140170636330173, 0.006296283178171225, 0.006643102217123409, 0.007348170114878397], 'accuracy': [0.6376536, 0.88706845, 0.93373317, 0.95011705, 0.9606495, 0.96664715, 0.97893506, 0.9856641, 0.9897601, 0.9951726, 0.995904, 0.99648917, 0.9982446, 0.9963429, 0.9970743, 0.998976, 0.9985372, 0.99956113, 0.9992686, 0.99956113, 0.99941486, 0.99970746, 0.99970746, 0.99868345, 0.9982446, 0.99868345, 0.99941486]}
# training and validation loss
loss_values = other['loss']
val_loss_values = other['val_loss']
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, 'b', label='Training loss')
plt.plot(epochs, val_loss_values, 'g', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
plt.clf()
# training and validation accuracy
acc_values = other['accuracy']
val_acc_values = other['val_accuracy']
plt.plot(epochs, acc_values, 'b', label='Training acc')
plt.plot(epochs, val_acc_values, 'g', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Acc')
plt.legend()
plt.show()
